{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n    \nimport os\nimport numpy as np\nfrom glob import glob\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport cv2\nfrom tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.layers import Conv2D , BatchNormalization , Activation , MaxPool2D , Input , Dropout , ZeroPadding2D , Conv2DTranspose , Concatenate\nfrom tensorflow.keras.applications import DenseNet201\n\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.applications import InceptionResNetV2\ndef conv_block(inputs , num_filters):\n  x = Conv2D(num_filters , 3 , padding= \"same\" )(inputs)\n  x = BatchNormalization()(x)\n  x = Activation(\"relu\")(x)\n  x = Dropout(0.2)(x)\n\n  x = Conv2D(num_filters , 3 , padding= \"same\" )(x)\n  x = BatchNormalization()(x)\n  x = Activation(\"relu\")(x)\n  x = Dropout(0.2)(x)\n  x = squeeze_excite_block(x)\n  return x\n\n\n\ndef decoder_block(inputs , skip_features , num_filters):\n\n  x = Conv2DTranspose(num_filters , (2, 2) , strides = 2 , padding = \"same\")(inputs)\n  x = Concatenate()([x , skip_features ])\n  x = conv_block(x , num_filters)\n\n  return x\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-18T06:49:54.730302Z","iopub.execute_input":"2021-06-18T06:49:54.73061Z","iopub.status.idle":"2021-06-18T06:50:00.779278Z","shell.execute_reply.started":"2021-06-18T06:49:54.730545Z","shell.execute_reply":"2021-06-18T06:50:00.778077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Even the skip features were extracted for the Decoder purposes to feed in the required Skip Features\n\ndef build_DenseNet210_Unet(input_shape):\n\n  ##Input\n  inputs = Input(input_shape)\n\n  ## Pre-Trained Encoder\n  encoder_feature = []\n    \n  encoder = DenseNet201(include_top = False , weights = \"imagenet\" , input_tensor = inputs)\n\n  # Skip Connections which are the Features and we will access these Features and Feed it in the Decoder\n  #First Feature is the input shaped image\n  s1 = encoder.get_layer(\"input_1\").output                         # 256 * 256\n\n  s2 = encoder.get_layer(\"conv1/relu\").output                       # 128 * 128 \n\n  s3 = encoder.get_layer(\"pool2_relu\").output       # 64 * 64 \n\n  s4 = encoder.get_layer(\"pool3_relu\").output       # 32 * 32\n  \n  for feature in [s1 , s2 , s3 , s4]:\n        encoder_feature.append(feature)\n  ## BottleNeck or bridge \n  b1 = encoder.get_layer(\"pool4_relu\").output       # 16 * 16\n\n  ## Decoder\n  d1 = decoder_block(b1 , s4 , 512)                                # 32 * 32 * 512 where 512 is the number of Features we extracted \n\n  d2 = decoder_block(d1 , s3 , 256)                                # 64 * 64\n\n  d3 = decoder_block(d2 , s2 , 128)                                # 128 * 128\n\n  d4 = decoder_block(d3 , s1 , 64)                                # 128 * 128\n\n  ## OUTPUT \n\n  outputs = Conv2D( 1 ,  1  ,  padding = \"same\" ,  activation = \"sigmoid\")(d4)\n\n  \n  return outputs , d4 , encoder_feature\n  #encoder.summary()\n\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:50:00.783779Z","iopub.execute_input":"2021-06-18T06:50:00.784116Z","iopub.status.idle":"2021-06-18T06:50:00.79733Z","shell.execute_reply.started":"2021-06-18T06:50:00.784082Z","shell.execute_reply":"2021-06-18T06:50:00.796348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:50:00.802373Z","iopub.execute_input":"2021-06-18T06:50:00.805028Z","iopub.status.idle":"2021-06-18T06:50:08.217631Z","shell.execute_reply.started":"2021-06-18T06:50:00.803062Z","shell.execute_reply":"2021-06-18T06:50:08.216746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nfrom keras.models import *\nfrom keras.layers import Input, Conv2D, UpSampling2D, BatchNormalization, Activation, add,average,concatenate\nfrom keras.layers.core import Lambda\nfrom keras.optimizers import *\nfrom keras.losses import binary_crossentropy\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, Multiply, AveragePooling2D, UpSampling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.applications import InceptionResNetV2\n\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\nimport cv2\nfrom tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.layers import Conv2D , BatchNormalization , Activation , MaxPool2D , Input , Dropout , ZeroPadding2D , Conv2DTranspose , Concatenate\n\n\ndef squeeze_excite_block(inputs, ratio=8):\n    init = inputs       ## (b, 128, 128, 32)\n    channel_axis = -1\n    filters = init.shape[channel_axis]\n    se_shape = (1, 1, filters)\n\n    se = GlobalAveragePooling2D()(init)     ## (b, 32)   -> (b, 1, 1, 32)\n    se = Reshape(se_shape)(se)\n    se = Dense(filters//ratio, activation=\"relu\", use_bias=False)(se)\n    se = Dense(filters, activation=\"sigmoid\", use_bias=False)(se)\n\n    x = Multiply()([inputs, se])\n    return x\n\ndef ASPP(x, filter):\n    shape = x.shape\n\n    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(x)\n    y1 = Conv2D(filter, 1, padding=\"same\")(y1)\n    y1 = BatchNormalization()(y1)\n    y1 = Activation(\"relu\")(y1)\n    y1 = Dropout(0.2)(y1)\n\n    y1 = UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y1)\n    \n    y2 = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(x)\n    y2 = BatchNormalization()(y2)\n    y2 = Activation(\"relu\")(y2)\n    y2 = Dropout(0.2)(y2)\n    \n    y3 = Conv2D(filter, 3, dilation_rate=6, padding=\"same\", use_bias=False)(x)\n    y3 = BatchNormalization()(y3)\n    y3 = Activation(\"relu\")(y3)\n    y3 = Dropout(0.2)(y3)\n    \n    y4 = Conv2D(filter, 3, dilation_rate=12, padding=\"same\", use_bias=False)(x)\n    y4 = BatchNormalization()(y4)\n    y4 = Activation(\"relu\")(y4)\n    y4 = Dropout(0.2)(y4)\n    \n    y5 = Conv2D(filter, 3, dilation_rate=18, padding=\"same\", use_bias=False)(x)\n    y5 = BatchNormalization()(y5)\n    y5 = Activation(\"relu\")(y5)\n    y5 = Dropout(0.2)(y5)\n    \n    y = Concatenate()([y1, y2, y3, y4, y5])\n\n    y = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(y)\n    y = BatchNormalization()(y)\n    y = Activation(\"relu\")(y)\n    y= Dropout(0.2)(y)\n    \n    return y\n\n\ndef encoder1(inputs):\n    skip_connections = []\n    model = InceptionResNetV2(include_top = False  , weights = \"imagenet\" , input_tensor = inputs)\n    #Skip Connection\n    s1 = model.get_layer(\"input_1\").output                 #Shape will be 256 * 256\n\n    s2 = model.get_layer(\"activation_2\").output         # 125 * 125 \n    s2 = ZeroPadding2D( ((2 , 1) ,  ( 2 , 1)) )(s2)          # 128 * 128\n\n\n    s3 = model.get_layer(\"activation_4\").output        # 60 * 60\n    s3 = ZeroPadding2D( ((2 , 2 ) , (2, 2)) )(s3)         # 64 * 64 \n\n\n    s4 = model.get_layer(\"activation_74\").output                              # 29*29\n    s4 = ZeroPadding2D( ((2 , 1) ,  ( 2 , 1)) )(s4) \n                                                                                # 32 * 32\n    # Padded with 2 on top , 1 in bottom     2 in left and 1 in right\n\n\n    b1 = model.get_layer(\"activation_161\").output                      # 14 * 14\n\n    #model = VGG19(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n    names = [\"input_1\", \"activation_2\", \"activation_4\", \"activation_74\"]\n\n    for feature in [s1 , s2 , s3 , s4]:\n\n        skip_connections.append(feature)\n\n\n    output = model.get_layer(\"activation_161\").output\n    output = ZeroPadding2D( ((1 , 1) , (1, 1)) )(output)                         # 16 * 16 \n\n    return output, skip_connections\n\ndef decoder1(inputs, skip_connections):\n    num_filters = [512,256, 128, 64]\n    skip_connections.reverse()\n\n    x = inputs\n    for i, f in enumerate(num_filters):\n        x = UpSampling2D((2, 2), interpolation=\"bilinear\")(x)\n        x = Concatenate()([x, skip_connections[i]])\n        x = conv_block(x, f)\n\n    return x\n\ndef output_block(inputs):\n    x = Conv2D(1, 1, padding=\"same\")(inputs)\n    x = Activation(\"sigmoid\")(x)\n    return x\n\ndef encoder2(inputs):\n    num_filters = [ 64, 128, 256 , 512]\n    skip_connections = []\n\n    x = inputs\n\n    for i, f in enumerate(num_filters):\n        x = conv_block(x, f)\n        skip_connections.append(x)\n        x = MaxPool2D((2, 2))(x)\n\n    return x, skip_connections\n\ndef decoder2(inputs, skip_1, skip_2 ,encoder2_skip_connection ):\n    num_filters = [512 , 256, 128, 64]\n    skip_2.reverse()\n    encoder2_skip_connection.reverse()\n    x = inputs\n    for i, f in enumerate(num_filters):\n        x = UpSampling2D((2, 2), interpolation=\"bilinear\")(x)\n        x = Concatenate()([x, skip_1[i], skip_2[i] , encoder2_skip_connection[i+1]])\n        #x = Concatenate()([x, skip_1[i], skip_2[i]])\n        x = conv_block(x, f)\n\n    return x\n\nh_heuns_method=0.5\n\ndef res_block(x, nb_filters, strides):\n    res_path = BatchNormalization()(x)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Conv2D(filters=nb_filters[0], kernel_size=(3, 3), padding='same' ,strides=strides[0])(res_path)\n    res_path = Dropout(0.2)(res_path)\n\n    res_path = BatchNormalization()(res_path)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Conv2D(filters=nb_filters[1], kernel_size=(3, 3), padding='same',strides=strides[1])(res_path)\n    \n    res_path = Dropout(0.2)(res_path)\n    \n    hpath = Lambda(lambda x: x * h_heuns_method)(res_path)\n\n    shortcut = Conv2D(nb_filters[1], kernel_size=(1, 1),strides=strides[0])(x)\n    shortcut = BatchNormalization()(shortcut)\n    shortcut = Activation(activation='relu')(shortcut)\n    shortcut = Dropout(0.2)(shortcut)\n    \n    res_path = add([shortcut, hpath])#suma corta\n    return res_path\n\ndef res_block2(x,y,nb_filters, strides):\n    res_path = BatchNormalization()(x)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Conv2D(filters=nb_filters[0], kernel_size=(3, 3), padding='same' ,strides=strides[0])(res_path)\n    res_path = Dropout(0.2)(res_path)\n\n    res_path = BatchNormalization()(res_path)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Conv2D(filters=nb_filters[1], kernel_size=(3, 3), padding='same',strides=strides[1])(res_path)\n    \n    res_path = Dropout(0.2)(res_path)\n\n    hpath = Lambda(lambda x: x * h_heuns_method)(res_path)\n\n    shortcut = Conv2D(nb_filters[1], kernel_size=(1, 1),strides=strides[0])(x)\n    shortcut = BatchNormalization()(shortcut)\n    #shortcut = Activation(activation='relu')(shortcut)\n    shortcut = Dropout(0.2)(shortcut)\n    res_path = add([shortcut, hpath])\n\n    res_path = average([y, res_path])\n    return res_path\n\ndef encoder(x):\n    to_decoder = []\n    main_path = Conv2D(filters=32, kernel_size=(3, 3), padding='same', strides=(1, 1))(x)  ## 512\n    main_path = BatchNormalization()(main_path)\n    main_path = Activation(activation='relu')(main_path)\n    main_path = Dropout(0.2)(main_path)\n\n    main_path = Conv2D(filters=32 , kernel_size=(3, 3), padding='same', strides=(1, 1))(main_path)## 512\n    main_path = BatchNormalization()(main_path)\n    main_path = Activation(activation='relu')(main_path)\n    main_path = Dropout(0.2)(main_path)\n    \n    hpath = Lambda(lambda x: x * h_heuns_method)(main_path)\n\n    shortcut = Conv2D(filters=32, kernel_size=(1, 1), strides=(1, 1))(x)                            ## 512\n    shortcut = BatchNormalization()(shortcut)\n    #shortcut = Activation(activation='relu')(shortcut)\n    shortcut = Dropout(0.2)(shortcut)\n\n    main_path = add([shortcut, hpath])    ## 512 is shortcut and 512 is hpath\n\n    to_decoder.append(main_path)\n\n\n    s1 = Conv2D(filters=64, kernel_size=(1, 1), strides=(2, 2))(x)#We can change Kernel Size                                 ## 512 to 256\n    s1 = BatchNormalization()(s1)\n    #s1 = Activation(activation='relu')(s1)\n    s1 = Dropout(0.2)(s1)\n    \n    main_path = res_block2(main_path,s1, [64, 64], [(2, 2), (1, 1)])                         # mainPath becames 512 to 256    to fit s1 which is 256\n    to_decoder.append(main_path)\n\n\n    s2 = Conv2D(filters=128, kernel_size=(1, 1), strides=(2, 2))(to_decoder[1])              # 256 to 128\n    s2 = BatchNormalization()(s2)\n    #s2 = Activation(activation='relu')(s2)\n    s2 = Dropout(0.2)(s2)\n    \n    main_path = res_block2(main_path,s2, [128, 128], [(2, 2), (1, 1)])                    # main_path became from 256 to 128 \n    to_decoder.append(main_path)\n    \n    s3 = Conv2D(filters=256, kernel_size=(1, 1), strides=(2, 2))(to_decoder[2])\n    s3 = BatchNormalization()(s3)\n    #s3 = Activation(activation='relu')(s3)\n    s3 = Dropout(0.2)(s3)\n    \n    main_path = res_block2(main_path,s3, [256, 256], [(2, 2), (1, 1)])                     # 128 to 64\n    to_decoder.append(main_path)\n\n    s4 = Conv2D(filters=512, kernel_size=(1, 1), strides=(2, 2))(to_decoder[3])        \n    s4 = BatchNormalization()(s4)\n    #s4 = Activation(activation='relu')(s4)\n    s4 = Dropout(0.2)(s4)\n    \n    \n    main_path = res_block2(main_path,s4, [512, 512], [(2, 2), (1, 1)])\n    to_decoder.append(main_path)\n\n    return to_decoder\n\ndef decoder(x, from_encoder):\n    main_path = UpSampling2D(size=(2, 2) , interpolation=\"bilinear\")(x)# 32 * 32\n    main_path1 = concatenate([main_path, from_encoder[-1]], axis=3)\n    main_path = res_block(main_path1, [512, 512], [(1, 1), (1, 1)])\n\n    main_path = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(main_path)###64x64\n    main_path = concatenate([main_path, from_encoder[3]], axis=3)#\n    u1 = UpSampling2D(size=(2, 2))(main_path1)#\n    u1 = Conv2D(256, kernel_size=(1, 1),strides=(1, 1))(u1)\n    u1 = BatchNormalization()(u1)   \n    #u1 = Activation(activation='relu')(u1)\n    u1 = Dropout(0.2)(u1)\n    \n    \n    main_path = res_block2(main_path,u1, [256, 256], [(1, 1), (1, 1)])#\n\n    main_path = UpSampling2D(size=(2, 2) , interpolation=\"bilinear\")(main_path)#128x128\n    main_path2 = concatenate([main_path, from_encoder[2]], axis=3)#\n    main_path = res_block(main_path2, [128, 128], [(1, 1), (1, 1)])#\n\n    main_path = UpSampling2D(size=(2, 2) , interpolation=\"bilinear\" )(main_path)#256x256\n    main_path = concatenate([main_path, from_encoder[1]], axis=3)#256x256\n\n    u2 = UpSampling2D(size=(2,2) , interpolation=\"bilinear\" )(main_path2)#\n    u2 = Conv2D(64, kernel_size=(1, 1),strides=(1, 1))(u2)#\n    u2 = BatchNormalization()(u2)\n    #u2 = Activation(activation='relu')(u2)\n    u2 = Dropout(0.2)(u2)\n    main_path3 = res_block2(main_path,u2, [64, 64], [(1, 1), (1, 1)])#256x256\n\n    \n    main_path = UpSampling2D(size=(2, 2) , interpolation=\"bilinear\" )(main_path3)#512 * 512\n    main_path = concatenate([main_path, from_encoder[0]], axis=3)# 512 * 512\n    \n    #u3 = UpSampling2D(size=(2,2) , interpolation=\"bilinear\")(main_path3)#\n    #u3 = Conv2D(64, kernel_size=(1, 1),strides=(1, 1))(u3)#\n    #u3 = BatchNormalization()(u3)\n    #u3 = Activation(activation='relu')(u3)\n    #u3 = Dropout(0.2)(u3)\n    \n    main_path = res_block(main_path, [ 64, 64], [(1, 1), (1, 1)])#\n\n\n    return main_path\n  \n\n\n\n\ndef build_model(input_shape):\n    inputs = Input(input_shape)\n    \n    output1 , features_map_Decoder1 , feature_map_Encoder1 = build_DenseNet210_Unet(inputs)\n    skip_1 = feature_map_Encoder1\n    #x, skip_1 = encoder1(inputs)\n    #x = ASPP(x, 64)\n    #x = decoder1(x, skip_1)\n    #output1 = output_block(x)\n\n    x1 = inputs * output1\n\n    #x = concatenate([x1 , features_map_Decoder1] , axis = 3)\n    # 2ND Encoder is taking Ouput of first Unet \n    to_decoder = encoder(x1)\n    path = res_block(to_decoder[-1], [1024, 1024], [(2, 2), (1, 1)])####bridge\n    #x = ASPP(x, 64)\n    path = decoder(path, from_encoder=to_decoder)\n    output2 = output_block(path)\n    x2 = inputs * output2\n    \n\n    x = concatenate([x1 , x2 ] , axis = 3)\n    x, skip_2 = encoder2(x)\n    x = ASPP(x, 64)\n    \n    x = decoder2(x, skip_1, skip_2 , to_decoder) ## Now Last Decoder is taking all the Features from Encoder 1 , Encoder 2 and Encoder 3 or say from Bridge 3 . Why it is not taking the outputs from Unet1 and Unet2\n    output3 = output_block(x)\n    #o1 = []\n    #o2 = []\n    #o3 = []\n    #o1.append(output1)\n    #o2.append(output2)\n    #o3.append(output3)\n    \n    \n    #outputs = concatenate([output1, output2 , output3, inputs] , axis=3)\n    \n    #output = output_block(outputs)\n    \n    model = Model(inputs, output3)\n    #return output , model\n    return model\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(paths , split = 0.15):\n  images = []\n  masks = []\n  numImages = 0\n  numMasks = 0\n\n  for path in paths:\n    \n        \n    images.extend(sorted(glob(os.path.join(path , \"images/*\"))))\n    masks.extend(sorted(glob(os.path.join(path , \"label/*\"))))\n    if \"training\" in path:\n      for i in [\"00\", \"01\" , \"02\" , \"03\" , \"04\" , \"05\" , \"07\", \"08\" , \"09\" , \"10\" , \"11\" , \"12\" , \"13\" , \"14\" , \"16\" , \"17\" , \"18\" , \"19\" , \"21\" , \"22\" , \"24\" , \"26\" , \"27\" ]:\n        images.remove(\"../input/chagas/ChagasTraining/training-20210525T143718Z-001/training/images/i8{}.xml\".format(i))\n\n      n = \"training\"\n    if \"Test\" in path:\n      n= \"Test\"\n    if \"Val\" in path:\n\n      n= \"Validation\"\n\n    print(\"{}PathImagesAre:\".format(n),len(images) - numImages)\n    print(\"{}PathImagesAre:\".format(n),len(masks) - numMasks)\n\n    numImages = len(images)\n    numMasks = len(masks)\n\n  total_size = len(images)\n  valid_size = int(split * total_size)\n  test_size = int(split * total_size)\n  print(\"Number of images in Total , Validation images and Test Images : \" , total_size , valid_size ,test_size  )\n\n  train_x , test_x = train_test_split(images , test_size = test_size , random_state = 42 )\n  train_y , test_y = train_test_split(masks , test_size = test_size , random_state = 42 )\n\n  train_x , valid_x = train_test_split(train_x , test_size = valid_size , random_state = 42 )\n  train_y , valid_y = train_test_split(train_y , test_size = valid_size , random_state = 42 )\n\n  return (train_x , train_y) , (valid_x , valid_y ) , (test_x , test_y) \n\ndef read_image(path):\n  path = path.decode()\n  x = cv2.imread(path , cv2.IMREAD_COLOR)\n  #x = cv2.resize(x , (256 , 256))\n  x = x/255.0\n  #Size is 256 * 256 * 3\n\n  return x\n\ndef read_mask(path):\n  path = path.decode()\n  x = cv2.imread(path , cv2.IMREAD_GRAYSCALE)\n  #x = cv2.resize(x , (256 , 256))\n  x = x/255.0\n  \n  #Size is 256 * 256\n  x = np.expand_dims(x , axis = -1)\n  #Size becames 256 * 256 * 1\n\n  return x\n\ndef tf_parse(imagepath , maskpath):\n  def _parse(imagepath , maskpath):\n    x = read_image(imagepath)\n    y = read_mask(maskpath)\n\n    return x , y\n\n  x , y = tf.numpy_function(_parse , [imagepath , maskpath] , [tf.float64 , tf.float64] )\n  x.set_shape([512 , 512 , 3])\n  y.set_shape([512, 512, 1])\n\n  return x , y \n\n\ndef tf_dataset( imagepath , maskpath , batch = 2):\n  dataset = tf.data.Dataset.from_tensor_slices((imagepath , maskpath))\n  dataset = dataset.map(tf_parse)\n  dataset = dataset.batch(batch)\n  dataset = dataset.repeat()\n  return dataset\n\nif __name__ == \"__main__\":\n  paths = [\"../input/chagas/ChagasTraining/training-20210525T143718Z-001/training\" , \"../input/chagas/ChagasTest/Test\", \"../input/chagas/ChagasValidation/Validation\"] \n  (train_x , train_y) , (valid_x , valid_y ) , (test_x , test_y)  =   load_data(paths)\n\n  print(\"Number of images in Train , Validation images and Test Images : \" ,len(train_x) , len(valid_x ) , len(test_x) )\n\n  ds = tf_dataset(test_x , test_y)\n  for x, y in ds:\n    \n    print(x.shape , y.shape)\n    break\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:50:08.219331Z","iopub.execute_input":"2021-06-18T06:50:08.219679Z","iopub.status.idle":"2021-06-18T06:50:09.367885Z","shell.execute_reply.started":"2021-06-18T06:50:08.21964Z","shell.execute_reply":"2021-06-18T06:50:09.367016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:50:09.368968Z","iopub.execute_input":"2021-06-18T06:50:09.369222Z","iopub.status.idle":"2021-06-18T06:50:09.376673Z","shell.execute_reply.started":"2021-06-18T06:50:09.369198Z","shell.execute_reply":"2021-06-18T06:50:09.375884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train \nimport os\nimport numpy as np\nimport cv2\nfrom glob import glob\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard\nfrom tensorflow.keras.metrics import Recall , Precision \n\ndef iou(y_true, y_pred):\n    def f(y_true, y_pred):\n        intersection = (y_true * y_pred).sum()\n        union = y_true.sum() + y_pred.sum() - intersection\n        x = (intersection + 1e-15) / (union + 1e-15)\n        x = x.astype(np.float32)\n        return x\n    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:50:09.378047Z","iopub.execute_input":"2021-06-18T06:50:09.378506Z","iopub.status.idle":"2021-06-18T06:50:09.386259Z","shell.execute_reply.started":"2021-06-18T06:50:09.37847Z","shell.execute_reply":"2021-06-18T06:50:09.38517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install segmentation_models","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:50:09.38739Z","iopub.execute_input":"2021-06-18T06:50:09.387822Z","iopub.status.idle":"2021-06-18T06:50:17.623248Z","shell.execute_reply.started":"2021-06-18T06:50:09.387788Z","shell.execute_reply":"2021-06-18T06:50:17.622324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env SM_FRAMEWORK=tf.keras","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:50:17.626993Z","iopub.execute_input":"2021-06-18T06:50:17.627272Z","iopub.status.idle":"2021-06-18T06:50:17.63668Z","shell.execute_reply.started":"2021-06-18T06:50:17.627241Z","shell.execute_reply":"2021-06-18T06:50:17.635689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    from segmentation_models.losses import bce_jaccard_loss\n    from segmentation_models.metrics import iou_score\n    np.random.seed(42)\n    tf.random.set_seed(42)\n    #HYPERPARAMETER\n\n\n    batch = 2\n    lr = 8.00E-05\n    epochs = 2\n    opt = tf.keras.optimizers.Adam(lr)\n    metrics = [\"acc\" , Recall() , Precision() , iou]\n    #Data\n    train_dataset = tf_dataset(train_x, train_y, batch=batch)\n    valid_dataset = tf_dataset(valid_x, valid_y, batch=batch)\n    model = build_model((512 , 512 , 3))\n    #final_output , model = build_model((512 , 512 , 3))\n    #model.summary()\n    #model.compile(loss=bce_jaccard_loss, optimizer=opt, metrics=metrics)\n    model.compile(optimizer= opt, loss=bce_jaccard_loss, metrics=[dice_loss,iou_coeff,precision,recall , ACL5])\n\n    callbacks = [\n        ModelCheckpoint(\"files/model.h5\"),\n        ReduceLROnPlateau(monitor='iou', factor=0.1, patience=4),\n        #CSVLogger(\"/content/drive/MyDrive/data.csv\"),\n        TensorBoard(),\n        EarlyStopping(monitor='iou', patience=15, restore_best_weights=False)\n    ]\n    #Number Of Batches\n    train_steps = len(train_x)//batch\n    valid_steps = len(valid_x)//batch\n\n    if len(train_x) % batch != 0:\n        train_steps += 1\n    if len(valid_x) % batch != 0:\n        valid_steps += 1\n\n    model.fit(train_dataset,\n        validation_data=valid_dataset,\n        epochs=epochs,\n        steps_per_epoch=train_steps,\n        validation_steps=valid_steps,\n        callbacks=callbacks,\n        shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:50:17.638432Z","iopub.execute_input":"2021-06-18T06:50:17.638895Z","iopub.status.idle":"2021-06-18T07:18:42.066865Z","shell.execute_reply.started":"2021-06-18T06:50:17.638846Z","shell.execute_reply":"2021-06-18T07:18:42.065957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    model.fit(train_dataset,\n        validation_data=valid_dataset,\n        epochs=30,\n        steps_per_epoch=train_steps,\n        validation_steps=valid_steps,\n        callbacks=callbacks,\n        shuffle = False)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:18:42.068434Z","iopub.execute_input":"2021-06-18T07:18:42.068806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n  \n# Directory\ndirectory = \"RESULT_DenseNet210WithDropout\"\n  \n# Parent Directory path\nparent_dir = \"./\"\n  \n# Path\npath = os.path.join(parent_dir, directory)\n  \n# Create the directory\n# 'GeeksForGeeks' in\n# '/home / User / Documents'\nos.mkdir(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.utils import CustomObjectScope\nfrom tqdm import tqdm\n#from data import load_data, tf_dataset\n#from train import iou\n\ndef read_image(path):\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (256, 256))\n    x = x/255.0\n    return x\n\ndef read_mask(path):\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (256, 256))\n    x = np.expand_dims(x, axis=-1)\n    return x\n\ndef mask_parse(mask):\n    mask = np.squeeze(mask)\n    mask = [mask, mask, mask]\n    mask = np.transpose(mask, (1, 2, 0))\n    return mask\n\nif __name__ == \"__main__\":\n    ## Dataset\n    \n    batch_size = 8\n    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n\n    test_dataset = tf_dataset(test_x, test_y, batch=batch_size)\n\n    test_steps = (len(test_x)//batch_size)\n    if len(test_x) % batch_size != 0:\n        test_steps += 1\n\n    #with CustomObjectScope({'iou': iou}):\n    #    model = tf.keras.models.load_model(\"/content/drive/model.h5\")\n\n    #model.evaluate(test_dataset, steps=test_steps)\n\n    for i, (x, y) in tqdm(enumerate(zip(test_x, test_y)), total=len(test_x)):\n        x = read_image(x)\n        y = read_mask(y)\n        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w, _ = x.shape\n        white_line = np.ones((h, 10, 3)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            mask_parse(y), white_line,\n            mask_parse(y_pred) * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        \n        cv2.imwrite(f\"./RESULT_DenseNet210WithDropout/{i}.png\", image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}